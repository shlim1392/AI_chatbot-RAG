{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d4d496",
   "metadata": {},
   "source": [
    "# **Transformer**\n",
    "## 1. Transformer가 등장한 이유\n",
    "\n",
    "|기존 구조|한계점|해결책 (Transformer)|\n",
    "|---|---|---|\n",
    "|**RNN / LSTM**|단어를 순서대로 처리 → 병렬 학습 불가|Attention 구조로 문맥 병렬 이해|\n",
    "|**CNN**|지역 단어 관계만 학습|전 문맥(Global context) 학습 가능|\n",
    "|**Transformer**|전체 문장을 동시에 입력받아, 단어 간의 **의미 관계**를 학습|문맥 이해 + 빠른 학습 + 대규모 데이터 처리|\n",
    "\n",
    "> 💬 한 문장 요약\n",
    "> \n",
    "> “Transformer는 **모든 단어가 서로의 의미를 바라보는(attend)** 구조다.”\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 2. Transformer의 핵심 구조\n",
    "![[image/Pasted image 20251023090919.png]]\n",
    "![[image/Pasted image 20251023091127.png]]\n",
    "### 전체 구조 개요\n",
    "\n",
    "```\n",
    "입력 문장 → 임베딩 → Self-Attention → Feed Forward → (반복 N회)\n",
    "\n",
    "```\n",
    "\n",
    "|구성요소|설명|\n",
    "|---|---|\n",
    "|**Embedding**|단어를 벡터(숫자)로 표현|\n",
    "|**Positional Encoding**|단어의 순서 정보를 더함|\n",
    "|**Self-Attention**|단어 간의 관계를 학습|\n",
    "|**Feed Forward**|비선형 변환으로 문맥 정제|\n",
    "|**Residual + LayerNorm**|정보 손실 방지 및 안정적 학습|\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 3. Self-Attention 이해하기 (GPT의 핵심)\n",
    "\n",
    "> 단어 간의 “관계 강도”를 계산하는 메커니즘\n",
    "> \n",
    "> 즉, 한 문장 내 단어들이 서로 **얼마나 관련 있는지**를 수치화함\n",
    "\n",
    "### 예시 문장\n",
    "\n",
    "> “The cat sat on the mat.”\n",
    "\n",
    "|단어|관계가 강하게 연결되는 단어|\n",
    "|---|---|\n",
    "|cat|sat, mat|\n",
    "|mat|on, cat|\n",
    "|the|거의 모든 단어와 약한 연결|\n",
    "\n",
    "### 💡 핵심 아이디어\n",
    "\n",
    "- 모델은 “cat”을 볼 때 “sat”, “mat”을 **같이 참조(attend)** 하여 문맥을 이해\n",
    "- 이런 방식으로 “순서”가 아닌 “의미 관계” 중심의 학습이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce97cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Tokenized: ['Deep', 'Ġlearning', 'Ġis', 'Ġamazing']\n",
      "🔹 Token IDs: [29744, 4673, 318, 4998]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Deep learning is amazing\"\n",
    "tokens = tok.tokenize(text)\n",
    "print(\"🔹 Tokenized:\", tokens)\n",
    "print(\"🔹 Token IDs:\", tok.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a645d",
   "metadata": {},
   "source": [
    "🔹 Tokenized: ['Deep', 'Ġlearning', 'Ġis', 'Ġamazing']\n",
    "🔹 Token IDs: [29744, 4673, 318, 4998]\n",
    "\n",
    "> Ġ = 공백을 의미   \n",
    "> 문맥 수준의 의미 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961a4dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1100750252fb4e6ca4976914827da482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shlim\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-tc-big-en-ko. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47e116bfe48455fa8f823d281110a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/815k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e80c5cfb494c4894b29db7a1aca007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ec48c0e87e4092b6bd41cc6e4a38ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0349c871eb42dabe30a2ed8dc4d1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shlim\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c663ed7b29384aa0adc82d962cde83c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed320e7bb28447aa1e162806790b4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feda7a99db4047a0aceb9bb327d3e9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaffb76e7dd8486b99ecacc97c4f3b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88f5cb93d4e470899fdff5980b147e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 준비 완료! 이제 바로 실습 시작합니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Hugging Face의 사전 학습된 모델 사용\n",
    "# (번역 / 문장 생성 / 요약 등 다양한 태스크)\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "print(\"✅ 준비 완료! 이제 바로 실습 시작합니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b00c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 English: Machine learning is transforming the way we live.\n",
      "🔸 Korean : Kei popular 잘 NBA 유니버설 안수.\n",
      "\n",
      "🔹 English: Artificial intelligence can help doctors diagnose diseases faster.\n",
      "🔸 Korean : Greece COMP은 그리스의 도시입니다.\n",
      "\n",
      "🔹 English: ChatGPT is changing how people learn and work.\n",
      "🔸 Korean : 06:02 그때 springs 대중적인 pleasingtruepatriot 끝 세그먼트.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_sentences = [\n",
    "    \"Machine learning is transforming the way we live.\",\n",
    "    \"Artificial intelligence can help doctors diagnose diseases faster.\",\n",
    "    \"ChatGPT is changing how people learn and work.\"\n",
    "]\n",
    "\n",
    "for s in english_sentences:\n",
    "    result = translator(s)[0]['translation_text']\n",
    "    print(f\"🔹 English: {s}\\n🔸 Korean : {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8c0085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Result:\n",
      " Rewrite the following sentence in a more formal way: 'AI is super cool and fun to use.' In fact, it's not. Instead, it's about fun.\n",
      "\n",
      "This isn't to say that AI is not fun, although it is certainly fun to use.\n",
      "\n",
      "AI is a very good way to go about your job.\n",
      "\n",
      "Why AI?\n",
      "\n",
      "AI is a very good way to go about your job. To put it simply, it's about fun.\n",
      "\n",
      "Because it's a good way to go about your job, and because it's a great way to go about your job. To put it simply, it's about fun.\n",
      "\n",
      "The problem with AI is that it's not very good at it. (In fact, in some cases, AI is often actually quite good at it.) It takes a lot of effort to make it work, and it's hard to use it to your advantage.\n",
      "\n",
      "But if you're not able to put much effort into making your job work, why not use the AI to your advantage? Here's why:\n",
      "\n",
      "It takes less time to learn the concepts of computation, and is much easier to understand, and to use.\n",
      "\n",
      "It takes less time to learn the concepts of computation, and is much easier to understand, and to use. It's easier to learn, which\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Rewrite the following sentence in a more formal way: 'AI is super cool and fun to use.'\"\n",
    "result = generator(prompt, max_length=40, num_return_sequences=1)[0]['generated_text']\n",
    "print(\"🧩 Result:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "516c9232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 English Summary:\n",
      " Artificial intelligence (AI) refers to machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind. \n",
      "\n",
      "🔸 Korean Translation:\n",
      " 은퇴                                                                                .                 .                                                .\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Artificial intelligence (AI) refers to the simulation of human intelligence\n",
    "in machines that are programmed to think like humans and mimic their actions.\n",
    "The term may also be applied to any machine that exhibits traits\n",
    "associated with a human mind such as learning and problem-solving.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: 영어 요약 (GPT 기반 Summarization)\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)[0]['summary_text']\n",
    "print(\"🔹 English Summary:\\n\", summary, \"\\n\")\n",
    "\n",
    "# Step 2: 요약된 문장을 한국어로 번역\n",
    "translation = translator(summary)[0]['translation_text']\n",
    "print(\"🔸 Korean Translation:\\n\", translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966773ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
