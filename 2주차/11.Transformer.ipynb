{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d4d496",
   "metadata": {},
   "source": [
    "# **Transformer**\n",
    "## 1. Transformerê°€ ë“±ì¥í•œ ì´ìœ \n",
    "\n",
    "|ê¸°ì¡´ êµ¬ì¡°|í•œê³„ì |í•´ê²°ì±… (Transformer)|\n",
    "|---|---|---|\n",
    "|**RNN / LSTM**|ë‹¨ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ â†’ ë³‘ë ¬ í•™ìŠµ ë¶ˆê°€|Attention êµ¬ì¡°ë¡œ ë¬¸ë§¥ ë³‘ë ¬ ì´í•´|\n",
    "|**CNN**|ì§€ì—­ ë‹¨ì–´ ê´€ê³„ë§Œ í•™ìŠµ|ì „ ë¬¸ë§¥(Global context) í•™ìŠµ ê°€ëŠ¥|\n",
    "|**Transformer**|ì „ì²´ ë¬¸ì¥ì„ ë™ì‹œì— ì…ë ¥ë°›ì•„, ë‹¨ì–´ ê°„ì˜ **ì˜ë¯¸ ê´€ê³„**ë¥¼ í•™ìŠµ|ë¬¸ë§¥ ì´í•´ + ë¹ ë¥¸ í•™ìŠµ + ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬|\n",
    "\n",
    "> ğŸ’¬ í•œ ë¬¸ì¥ ìš”ì•½\n",
    "> \n",
    "> â€œTransformerëŠ” **ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œì˜ ì˜ë¯¸ë¥¼ ë°”ë¼ë³´ëŠ”(attend)** êµ¬ì¡°ë‹¤.â€\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ 2. Transformerì˜ í•µì‹¬ êµ¬ì¡°\n",
    "![[image/Pasted image 20251023090919.png]]\n",
    "![[image/Pasted image 20251023091127.png]]\n",
    "### ì „ì²´ êµ¬ì¡° ê°œìš”\n",
    "\n",
    "```\n",
    "ì…ë ¥ ë¬¸ì¥ â†’ ì„ë² ë”© â†’ Self-Attention â†’ Feed Forward â†’ (ë°˜ë³µ NíšŒ)\n",
    "\n",
    "```\n",
    "\n",
    "|êµ¬ì„±ìš”ì†Œ|ì„¤ëª…|\n",
    "|---|---|\n",
    "|**Embedding**|ë‹¨ì–´ë¥¼ ë²¡í„°(ìˆ«ì)ë¡œ í‘œí˜„|\n",
    "|**Positional Encoding**|ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ë¥¼ ë”í•¨|\n",
    "|**Self-Attention**|ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµ|\n",
    "|**Feed Forward**|ë¹„ì„ í˜• ë³€í™˜ìœ¼ë¡œ ë¬¸ë§¥ ì •ì œ|\n",
    "|**Residual + LayerNorm**|ì •ë³´ ì†ì‹¤ ë°©ì§€ ë° ì•ˆì •ì  í•™ìŠµ|\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” 3. Self-Attention ì´í•´í•˜ê¸° (GPTì˜ í•µì‹¬)\n",
    "\n",
    "> ë‹¨ì–´ ê°„ì˜ â€œê´€ê³„ ê°•ë„â€ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜\n",
    "> \n",
    "> ì¦‰, í•œ ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì´ ì„œë¡œ **ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€**ë¥¼ ìˆ˜ì¹˜í™”í•¨\n",
    "\n",
    "### ì˜ˆì‹œ ë¬¸ì¥\n",
    "\n",
    "> â€œThe cat sat on the mat.â€\n",
    "\n",
    "|ë‹¨ì–´|ê´€ê³„ê°€ ê°•í•˜ê²Œ ì—°ê²°ë˜ëŠ” ë‹¨ì–´|\n",
    "|---|---|\n",
    "|cat|sat, mat|\n",
    "|mat|on, cat|\n",
    "|the|ê±°ì˜ ëª¨ë“  ë‹¨ì–´ì™€ ì•½í•œ ì—°ê²°|\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì•„ì´ë””ì–´\n",
    "\n",
    "- ëª¨ë¸ì€ â€œcatâ€ì„ ë³¼ ë•Œ â€œsatâ€, â€œmatâ€ì„ **ê°™ì´ ì°¸ì¡°(attend)** í•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´\n",
    "- ì´ëŸ° ë°©ì‹ìœ¼ë¡œ â€œìˆœì„œâ€ê°€ ì•„ë‹Œ â€œì˜ë¯¸ ê´€ê³„â€ ì¤‘ì‹¬ì˜ í•™ìŠµì´ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce97cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Tokenized: ['Deep', 'Ä learning', 'Ä is', 'Ä amazing']\n",
      "ğŸ”¹ Token IDs: [29744, 4673, 318, 4998]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Deep learning is amazing\"\n",
    "tokens = tok.tokenize(text)\n",
    "print(\"ğŸ”¹ Tokenized:\", tokens)\n",
    "print(\"ğŸ”¹ Token IDs:\", tok.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a645d",
   "metadata": {},
   "source": [
    "ğŸ”¹ Tokenized: ['Deep', 'Ä learning', 'Ä is', 'Ä amazing']\n",
    "ğŸ”¹ Token IDs: [29744, 4673, 318, 4998]\n",
    "\n",
    "> Ä  = ê³µë°±ì„ ì˜ë¯¸   \n",
    "> ë¬¸ë§¥ ìˆ˜ì¤€ì˜ ì˜ë¯¸ íŒŒì•…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961a4dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1100750252fb4e6ca4976914827da482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shlim\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-tc-big-en-ko. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47e116bfe48455fa8f823d281110a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/815k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e80c5cfb494c4894b29db7a1aca007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ec48c0e87e4092b6bd41cc6e4a38ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0349c871eb42dabe30a2ed8dc4d1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shlim\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c663ed7b29384aa0adc82d962cde83c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed320e7bb28447aa1e162806790b4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feda7a99db4047a0aceb9bb327d3e9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaffb76e7dd8486b99ecacc97c4f3b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88f5cb93d4e470899fdff5980b147e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ë°”ë¡œ ì‹¤ìŠµ ì‹œì‘í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Hugging Faceì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©\n",
    "# (ë²ˆì—­ / ë¬¸ì¥ ìƒì„± / ìš”ì•½ ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬)\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "print(\"âœ… ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ë°”ë¡œ ì‹¤ìŠµ ì‹œì‘í•©ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b00c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ English: Machine learning is transforming the way we live.\n",
      "ğŸ”¸ Korean : Kei popular ì˜ NBA ìœ ë‹ˆë²„ì„¤ ì•ˆìˆ˜.\n",
      "\n",
      "ğŸ”¹ English: Artificial intelligence can help doctors diagnose diseases faster.\n",
      "ğŸ”¸ Korean : Greece COMPì€ ê·¸ë¦¬ìŠ¤ì˜ ë„ì‹œì…ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ”¹ English: ChatGPT is changing how people learn and work.\n",
      "ğŸ”¸ Korean : 06:02 ê·¸ë•Œ springs ëŒ€ì¤‘ì ì¸ pleasingtruepatriot ë ì„¸ê·¸ë¨¼íŠ¸.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_sentences = [\n",
    "    \"Machine learning is transforming the way we live.\",\n",
    "    \"Artificial intelligence can help doctors diagnose diseases faster.\",\n",
    "    \"ChatGPT is changing how people learn and work.\"\n",
    "]\n",
    "\n",
    "for s in english_sentences:\n",
    "    result = translator(s)[0]['translation_text']\n",
    "    print(f\"ğŸ”¹ English: {s}\\nğŸ”¸ Korean : {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8c0085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Result:\n",
      " Rewrite the following sentence in a more formal way: 'AI is super cool and fun to use.' In fact, it's not. Instead, it's about fun.\n",
      "\n",
      "This isn't to say that AI is not fun, although it is certainly fun to use.\n",
      "\n",
      "AI is a very good way to go about your job.\n",
      "\n",
      "Why AI?\n",
      "\n",
      "AI is a very good way to go about your job. To put it simply, it's about fun.\n",
      "\n",
      "Because it's a good way to go about your job, and because it's a great way to go about your job. To put it simply, it's about fun.\n",
      "\n",
      "The problem with AI is that it's not very good at it. (In fact, in some cases, AI is often actually quite good at it.) It takes a lot of effort to make it work, and it's hard to use it to your advantage.\n",
      "\n",
      "But if you're not able to put much effort into making your job work, why not use the AI to your advantage? Here's why:\n",
      "\n",
      "It takes less time to learn the concepts of computation, and is much easier to understand, and to use.\n",
      "\n",
      "It takes less time to learn the concepts of computation, and is much easier to understand, and to use. It's easier to learn, which\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Rewrite the following sentence in a more formal way: 'AI is super cool and fun to use.'\"\n",
    "result = generator(prompt, max_length=40, num_return_sequences=1)[0]['generated_text']\n",
    "print(\"ğŸ§© Result:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "516c9232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ English Summary:\n",
      " Artificial intelligence (AI) refers to machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind. \n",
      "\n",
      "ğŸ”¸ Korean Translation:\n",
      " ì€í‡´                                                                                .                 .                                                .\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Artificial intelligence (AI) refers to the simulation of human intelligence\n",
    "in machines that are programmed to think like humans and mimic their actions.\n",
    "The term may also be applied to any machine that exhibits traits\n",
    "associated with a human mind such as learning and problem-solving.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: ì˜ì–´ ìš”ì•½ (GPT ê¸°ë°˜ Summarization)\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)[0]['summary_text']\n",
    "print(\"ğŸ”¹ English Summary:\\n\", summary, \"\\n\")\n",
    "\n",
    "# Step 2: ìš”ì•½ëœ ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­\n",
    "translation = translator(summary)[0]['translation_text']\n",
    "print(\"ğŸ”¸ Korean Translation:\\n\", translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966773ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
