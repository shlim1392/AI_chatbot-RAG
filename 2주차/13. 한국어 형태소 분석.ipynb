{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d87e2f2",
   "metadata": {},
   "source": [
    "# **í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„**\n",
    "\n",
    "### 1. í˜•íƒœì†Œ ë¶„ì„ì´ë€?\n",
    "\n",
    "> í˜•íƒœì†Œ(morpheme): ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ê°€ì¥ ì‘ì€ ë‹¨ìœ„\n",
    "> \n",
    "> ì˜ˆ:\n",
    "> \n",
    "> - â€œì¬ë¯¸ìˆì—ˆë‹¤â€ â†’ `ì¬ë¯¸ / ìˆ / ì—ˆ / ë‹¤`\n",
    "> - â€œë°°ì†¡ì´ ë¹ ë¥´ë„¤ìš”â€ â†’ `ë°°ì†¡ / ì´ / ë¹ ë¥´ / ë„¤ìš”`\n",
    "\n",
    "|êµ¬ë¶„|ì˜ì–´(Word Tokenization)|í•œêµ­ì–´(í˜•íƒœì†Œ ë¶„ì„)|\n",
    "|---|---|---|\n",
    "|ë‹¨ì–´ ë‹¨ìœ„|ê³µë°± ê¸°ì¤€ ë¶„ë¦¬ ê°€ëŠ¥|ì¡°ì‚¬, ì–´ë¯¸ ë“± ë•Œë¬¸ì— ë³µì¡|\n",
    "|ì˜ˆì‹œ|â€œI love movieâ€ â†’ [I, love, movie]|â€œì˜í™”ë¥¼ ì¢‹ì•„í•œë‹¤â€ â†’ [ì˜í™”, ë¥¼, ì¢‹ì•„, í•œë‹¤]|\n",
    "|ë¬¸ì œì |ì—†ìŒ|â€œì¢‹ì•„â€, â€œì¢‹ì•„ìš”â€, â€œì¢‹ì•˜ë‹¤â€ â†’ í˜•íƒœê°€ ë‹¤ë¦„|\n",
    "\n",
    "ğŸ‘‰ ë”°ë¼ì„œ í•œêµ­ì–´ì—ì„œëŠ” **í˜•íƒœì†Œ ë¶„ì„ê¸°**ë¥¼ ì´ìš©í•´ì•¼ ë¬¸ì¥ì„ ì˜¬ë°”ë¥´ê²Œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§© 2. Okt(Open Korean Text) ì†Œê°œ\n",
    "\n",
    "|í•­ëª©|ì„¤ëª…|\n",
    "|---|---|\n",
    "|**ì´ë¦„**|Open Korean Text (êµ¬ Twitter í•œêµ­ì–´ ë¶„ì„ê¸°)|\n",
    "|**ì œê³µ ëª¨ë“ˆ**|`konlpy.tag.Okt()`|\n",
    "|**íŠ¹ì§•**|í•œêµ­ì–´ ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , í’ˆì‚¬ íƒœê¹… ë° ì–´ê°„(stem) ì¶”ì¶œ ê°€ëŠ¥|\n",
    "|**ìì£¼ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ**|`.morphs()`, `.pos()`, `.nouns()`|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af2168",
   "metadata": {},
   "source": [
    "> ìœ ì‚¬í•œ í•˜ìœ„í˜¸í™˜  mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1831815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ í˜•íƒœì†Œ: ['ì´', 'ì˜í™”', 'ëŠ”', 'ì •ë§', 'ì¬ë¯¸ìˆê³ ', 'ê°ë™', 'ì ', 'ì´ì—ˆì–´ìš”', '!']\n",
      "ğŸ”¹ ëª…ì‚¬ë§Œ: ['ì´', 'ì˜í™”', 'ì •ë§', 'ê°ë™']\n",
      "ğŸ”¹ í’ˆì‚¬ íƒœê¹…: [('ì´', 'Noun'), ('ì˜í™”', 'Noun'), ('ëŠ”', 'Josa'), ('ì •ë§', 'Noun'), ('ì¬ë¯¸ìˆê³ ', 'Adjective'), ('ê°ë™', 'Noun'), ('ì ', 'Suffix'), ('ì´ì—ˆì–´ìš”', 'Verb'), ('!', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "sentence = \"ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆê³  ê°ë™ì ì´ì—ˆì–´ìš”!\"\n",
    "print(\"ğŸ”¹ í˜•íƒœì†Œ:\", okt.morphs(sentence))\n",
    "print(\"ğŸ”¹ ëª…ì‚¬ë§Œ:\", okt.nouns(sentence))\n",
    "print(\"ğŸ”¹ í’ˆì‚¬ íƒœê¹…:\", okt.pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45346031",
   "metadata": {},
   "source": [
    "###  3. ë¶ˆìš©ì–´ ì œê±° + ì–´ê°„ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5098949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì „ì²˜ë¦¬ ì „: ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆê³  ê°ë™ì ì´ì—ˆì–´ìš”!\n",
      "âœ… ì „ì²˜ë¦¬ í›„: ì˜í™” ì •ë§ ì¬ë¯¸ìˆë‹¤ ê°ë™\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì˜','ì™€','ê³¼',\n",
    "             'ë„','ìœ¼ë¡œ','ë¡œ','ì—ì„œ','ë¼','í•˜ë‹¤','ìˆë‹¤','ë˜ë‹¤','ì´ë‹¤','ê·¸','ì €','ê²ƒ']\n",
    "\n",
    "def tokenize_korean(text):\n",
    "    tokens = okt.morphs(str(text), stem=True)        # ì–´ê°„ì¶”ì¶œ(stem=True)\n",
    "    tokens = [w for w in tokens if w not in stopwords and len(w) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "sample = \"ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆê³  ê°ë™ì ì´ì—ˆì–´ìš”!\"\n",
    "print(\"âœ… ì „ì²˜ë¦¬ ì „:\", sample)\n",
    "print(\"âœ… ì „ì²˜ë¦¬ í›„:\", tokenize_korean(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29f911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë‹¨ì–´ ëª©ë¡: ['ê°ë™' 'ë„ˆë¬´' 'ë°°ìš°' 'ìŠ¤í† ë¦¬' 'ì—°ê¸°' 'ì˜í™”' 'ìŒì•…' 'ì´ì—ìš”' 'ì¬ë¯¸ìˆë‹¤' 'ì •ë§' 'ì§€ë£¨í•˜ë‹¤' 'ìµœì•…']\n",
      "\\nâœ… ë³€í™˜ëœ TF-IDF í–‰ë ¬:\\n [[0.         0.         0.         0.         0.         0.57735027\n",
      "  0.         0.         0.57735027 0.57735027 0.         0.        ]\n",
      " [0.         0.         0.5        0.         0.5        0.\n",
      "  0.         0.5        0.         0.         0.         0.5       ]\n",
      " [0.70710678 0.         0.         0.         0.         0.\n",
      "  0.70710678 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.57735027 0.         0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.57735027 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„°\n",
    "data = [\n",
    "    \"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì–´ìš”\",\n",
    "    \"ë°°ìš° ì—°ê¸°ê°€ ìµœì•…ì´ì—ìš”\",\n",
    "    \"ìŒì•…ì´ ê°ë™ì ì´ì—ˆì–´ìš”\",\n",
    "    \"ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ì§€ë£¨í–ˆì–´ìš”\",\n",
    "]\n",
    "\n",
    "# 1ï¸âƒ£ í˜•íƒœì†Œ ë¶„ì„ ë° ì •ì œ\n",
    "tokenized = [tokenize_korean(t) for t in data]\n",
    "\n",
    "# 2ï¸âƒ£ TF-IDF ë³€í™˜\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vec = vectorizer.fit_transform(tokenized)\n",
    "\n",
    "print(\"âœ… ë‹¨ì–´ ëª©ë¡:\", vectorizer.get_feature_names_out())\n",
    "print(\"\\\\nâœ… ë³€í™˜ëœ TF-IDF í–‰ë ¬:\\\\n\", X_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595e212",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
