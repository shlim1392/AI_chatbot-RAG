{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e0b693",
   "metadata": {},
   "source": [
    "# **embedding과 rnn**\n",
    "\n",
    "## 1. 이론 정리\n",
    "\n",
    "### (1) 임베딩(Embedding)이란?\n",
    "![[image/Pasted image 20251022131229.png]]\n",
    "> 단어를 실수 벡터로 변환하는 학습 가능한 레이어\n",
    "\n",
    "|구분|원-핫 인코딩|임베딩|\n",
    "|---|---|---|\n",
    "|벡터 형태|[0, 0, 1, 0, 0, ...]|[0.12, -0.45, 0.87, ...]|\n",
    "|차원|어휘 수만큼 (희소)|저차원 (밀집)|\n",
    "|의미 반영|X|✅ 유사 단어끼리 벡터가 가까움|\n",
    "\n",
    "**핵심:** 임베딩 레이어는 **학습 중 자동으로 단어 의미를 학습**한다.\n",
    "\n",
    "즉, \"좋다\"와 \"훌륭하다\"의 벡터가 점점 비슷해진다.\n",
    "\n",
    "---\n",
    "\n",
    "### (2) RNN(Recurrent Neural Network)이란?\n",
    "![[image/Pasted image 20251022131356.png]]\n",
    "> 데이터를 “순서대로” 읽어가며 문맥을 학습하는 네트워크\n",
    "\n",
    "- 입력 시퀀스: 단어들의 <span style=\"background:#fff88f\">**순서 있는 배열**</span>\n",
    "- 은닉 상태(hidden state): 이전 단어의 정보를 다음으로 전달\n",
    "- 주요 장점: 문맥(순서) 반영\n",
    "- 한계: 긴 문장에서는 기억 손실 발생 → LSTM/GRU로 개선\n",
    "\n",
    "---\n",
    "\n",
    "### (3) Embedding + RNN 모델 구조\n",
    "\n",
    "```\n",
    "입력(단어 시퀀스)\n",
    "   ↓\n",
    "Embedding Layer (단어 → 벡터)\n",
    "   ↓\n",
    "RNN Layer (순서 정보 학습)\n",
    "   ↓\n",
    "Dense Layer (출력층, 분류)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80172f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0) 라이브러리 불러오기\n",
    "# ==============================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer        # 텍스트 → 정수 인코딩\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # 시퀀스 길이 맞추기(패딩)\n",
    "from tensorflow.keras import Sequential                           # 순차적 모델 구성 클래스\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense   # 신경망 주요 레이어\n",
    "\n",
    "# ==============================\n",
    "# 1) 미니 데이터 준비\n",
    "# ==============================\n",
    "# 영화 리뷰 데이터(한글 간단 문장 8개)\n",
    "# 1 = 긍정(Positive), 0 = 부정(Negative)\n",
    "texts = [\n",
    "    \"이 영화 정말 재미있다\",\n",
    "    \"배우 연기가 훌륭하다\",\n",
    "    \"감동적인 스토리에 눈물이 났다\",\n",
    "    \"추천하고 싶은 영화다\",\n",
    "    \"최악이다 다시 보고 싶지 않다\",\n",
    "    \"지루하고 시간 낭비였다\",\n",
    "    \"스토리가 엉성하고 별로다\",\n",
    "    \"다시는 보고 싶지 않다\"\n",
    "]\n",
    "labels = [1, 1, 1, 1, 0, 0, 0, 0]  # 감성 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd2620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 시퀀스 예시:\n",
      " [[ 5  6  7  8  0  0  0  0]\n",
      " [ 9 10 11  0  0  0  0  0]]\n",
      "단어 인덱스: {'<OOV>': 1, '보고': 2, '싶지': 3, '않다': 4, '이': 5, '영화': 6, '정말': 7, '재미있다': 8, '배우': 9, '연기가': 10, '훌륭하다': 11, '감동적인': 12, '스토리에': 13, '눈물이': 14, '났다': 15, '추천하고': 16, '싶은': 17, '영화다': 18, '최악이다': 19, '다시': 20, '지루하고': 21, '시간': 22, '낭비였다': 23, '스토리가': 24, '엉성하고': 25, '별로다': 26, '다시는': 27}\n"
     ]
    }
   ],
   "source": [
    "#  토크나이징 & 패딩\n",
    "vocab_size = 1000 # 최대 단어 수 (단어사전 크기)\n",
    "maxlen = 8 # 문장의 최대길이\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")  # OOV : out of vocab, 사전의 없는 단어는 oov로 대체됨\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 정수 시퀀스로 변경\n",
    "# 가장 빈도가 높은 순서대로 숫자 배정 -> 데이터 순서대로 매핑됨\n",
    "seqs = tokenizer.texts_to_sequences(texts) \n",
    "\n",
    "X = pad_sequences(seqs, maxlen=maxlen, padding='post', truncating='post')\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"샘플 시퀀스 예시:\\n\", X[:2])\n",
    "print(\"단어 인덱스:\", tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c06e5",
   "metadata": {},
   "source": [
    "> padding 모든 문장의 길이를 맞추는 구문  \n",
    "> [5, 6, 7, 8, 0, 0]  \n",
    "> [9, 10, 11, 0, 0]  \n",
    "> truncating 문장이 너무 긴 경우에는 최대 8글자였기 때문에 8자로 자름  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51fb4c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shlim\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 구성 / 컴파일\n",
    "# embedding과 rnn\n",
    "\n",
    "embedding_dim = 16 # 단어를 16차원으로 임베딩\n",
    "# 임베딩 [1,2,3] -> [[0.21, -0.17, 0.56..]]\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
    "    SimpleRNN(units=32),\n",
    "    Dense(1, activation= \"sigmoid\") # 0 과 1사이의 확률값으로 산출하는 1개의 뉴런\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d86813a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05372df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7500 - loss: 0.6839\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.6583 \n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.6195 \n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.5810 \n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.5336 \n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.4744 \n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3972 \n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3082 \n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.2122 \n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1308 \n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0742 \n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0450 \n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0297 \n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0221 \n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0177 \n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0150 \n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0131 \n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0117 \n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0107 \n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0098 \n"
     ]
    }
   ],
   "source": [
    "# 모델학습\n",
    "history = model.fit(X,y, epochs=20, batch_size=2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2314b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\n",
      "🔹 예측 결과:\n",
      "정말 감동적이고 훌륭한 영화 → 긍정😀\n",
      "별로야 지루했어 → 부정😞\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "test_texts = [\"정말 감동적이고 훌륭한 영화\", \"별로야 지루했어\"]\n",
    "test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "test_pad = pad_sequences(test_seq, maxlen=maxlen, padding='post')\n",
    "pred = (model.predict(test_pad) > 0.5).astype(int).ravel()\n",
    "\n",
    "print(\"\\n🔹 예측 결과:\")\n",
    "for t, p in zip(test_texts, pred):\n",
    "    print(f\"{t} → {'긍정😀' if p==1 else '부정😞'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ce59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
