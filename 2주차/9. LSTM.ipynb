{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7803a7ee",
   "metadata": {},
   "source": [
    "# **LSTM**\n",
    "## 이론\n",
    "![[image/Pasted image 20251022141148.png]]\n",
    "> 입출력 기록에 대한 부분을 보완함(RNN의 단점 극복을 위함)\n",
    "### 1) LSTM(Long Short-Term Memory)\n",
    "\n",
    "- **문제 배경**: 단순 RNN은 긴 문장에서 **기울기 소실**로 초반 정보가 사라지기 쉬움.\n",
    "- **핵심 아이디어**: “**셀 상태(cell state)**”라는 **고속도로**를 유지하면서 필요한 정보만 게이트로 **추가·제거**.\n",
    "- **세 가지 게이트**\n",
    "    - **망각 게이트** (f_t): 무엇을 **버릴지** 결정\n",
    "    - **입력 게이트** (i_t): 무엇을 **새로 담을지** 결정 (후보 ( \\tilde{C}_t ))\n",
    "    - **출력 게이트** (o_t): 어떤 **은닉 상태**를 다음으로 보낼지 결정\n",
    "- **효과**: 먼 과거의 신호를 **오래 보존**하면서도, 불필요한 정보는 버릴 수 있음 → 장기 의존성 해결.\n",
    "\n",
    "### 2) Bidirectional(양방향) RNN/LSTM\n",
    "![[image/Pasted image 20251022141344.png]]\n",
    "- **아이디어**: 문장을 **왼→오(정방향)**, **오→왼(역방향)**으로 **동시에** 읽음.\n",
    "- **효과**: 현재 단어 예측 시 **앞·뒤 문맥** 모두 활용 → 의존성이 양방향인 태스크(감성·개체명 인식 등)에서 유리.\n",
    "- **주의**: 실시간 스트리밍처럼 미래를 볼 수 없는 경우에는 적용 불가.\n",
    "\n",
    "### 3) Dropout (과적합 완화)\n",
    "![[image/Pasted image 20251022141445.png]]\n",
    "> 너무 많은 가중치가 연산을 방해할 수 있음 -> 중요하지 않은 뉴런을 죽임\n",
    "> -> 효율성 향상/과적합 대응력 향상/학습속도 향상\n",
    "\n",
    "\n",
    "- **일반 Dropout**: 레이어 출력을 학습 중 확률적으로 **0**으로 만들어 **공동 적응(co-adaptation)** 을 방지.\n",
    "- **Recurrent Dropout**: 순환(은닉→은닉) 연결에도 드롭아웃을 적용.\n",
    "- **Keras 적용 위치**\n",
    "    - `Dropout(p)`: Embedding 뒤, LSTM 뒤 등 **레이어 사이**\n",
    "    - `LSTM(..., dropout=p, recurrent_dropout=q)`: LSTM 내부 입력·순환 연결에 직접 적용\n",
    "- **팁**: `recurrent_dropout`은 GPU에서 느릴 수 있음. 처음에는 **외부 Dropout**부터 적용 → 필요 시 `recurrent_dropout` 소량(0.1~0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d8a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기: (12, 10)\n",
      "검증 데이터 크기: (4, 10)\n",
      "✅ 모델별 성능(소형 데이터, 참고용)\n",
      "       Base LSTM | Train=1.000 | Val=0.500\n",
      "  LSTM + Dropout | Train=0.750 | Val=0.500\n",
      "          BiLSTM | Train=1.000 | Val=0.250\n",
      "   Base 예측: [('정말 감동적이고 훌륭한 영화', np.int64(1)), ('지루하고 별로였어 다시 안봐', np.int64(0))]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021C45B08C20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Dropout 예측: [('정말 감동적이고 훌륭한 영화', np.int64(1)), ('지루하고 별로였어 다시 안봐', np.int64(0))]\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021C45B0AE80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " BiLSTM 예측: [('정말 감동적이고 훌륭한 영화', np.int64(1)), ('지루하고 별로였어 다시 안봐', np.int64(0))]\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 0) 환경\n",
    "# ======================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ======================\n",
    "# 1) 미니 데이터\n",
    "# ======================\n",
    "texts = [\n",
    "    \"이 영화 정말 재미있다\", \"배우 연기가 훌륭하다\", \"감동적인 스토리에 눈물이 났다\", \"완전 추천한다 최고다\",\n",
    "    \"음악과 영상미가 너무 좋았다\", \"유머가 자연스럽고 몰입됐다\", \"따뜻하고 여운이 긴 작품\", \"감독의 연출이 인상적이다\",\n",
    "    \"최악이다 돈이 아깝다\", \"지루하고 시간 낭비였다\", \"스토리가 엉성하고 별로다\", \"다시는 보고 싶지 않다\",\n",
    "    \"캐릭터가 매력 없고 산만했다\", \"전개가 느리고 답답했다\", \"웃음 포인트가 전혀 없었다\", \"실망스러운 마무리였다\"\n",
    "]\n",
    "labels = [1,1,1,1, 1,1,1,1, 0,0,0,0, 0,0,0,0]\n",
    "\n",
    "# ======================\n",
    "# 2) 토크나이징 & 패딩\n",
    "# ======================\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 2000\n",
    "maxlen = 10\n",
    "\n",
    "tok = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tok.fit_on_texts(texts)\n",
    "\n",
    "# 문장 → 시퀀스 → 패딩\n",
    "X = pad_sequences(tok.texts_to_sequences(texts), maxlen=maxlen, padding=\"post\")\n",
    "y = np.array(labels)\n",
    "\n",
    "# ======================\n",
    "# 3) 데이터 분리 (train/test)\n",
    "# ======================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"훈련 데이터 크기:\", X_train.shape)\n",
    "print(\"검증 데이터 크기:\", X_val.shape)\n",
    "\n",
    "# ======================\n",
    "# 3) 모델 빌더\n",
    "# ======================\n",
    "embedding_dim = 32\n",
    "units = 32\n",
    "\n",
    "# --- (1) 기본 LSTM 모델 ---\n",
    "def build_base():\n",
    "    m = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim),   # input_length 제거 (자동 인식)\n",
    "        LSTM(units),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# --- (2) Dropout 추가 LSTM ---\n",
    "def build_dropout():\n",
    "    m = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim),\n",
    "        Dropout(0.2),                 # Embedding 출력 정규화\n",
    "        LSTM(units, dropout=0.2),     # 내부 입력 드롭아웃\n",
    "        Dropout(0.2),                 # LSTM 출력 정규화\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# --- (3) 양방향 LSTM ---\n",
    "def build_bilstm():\n",
    "    m = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim),\n",
    "        Bidirectional(LSTM(units)),   # 순방향 + 역방향 학습\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 4) 학습 & 리포트\n",
    "# ======================\n",
    "def train(name, model):\n",
    "    \"\"\"\n",
    "    모델을 학습시키고, 마지막 epoch의 train/val 정확도를 출력\n",
    "    \"\"\"\n",
    "    h = model.fit(\n",
    "        X_train, y_train,             # ✅ 변수명 수정됨\n",
    "        epochs=12, batch_size=4,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    print(f\"{name:>16} | Train={h.history['accuracy'][-1]:.3f} | Val={h.history['val_accuracy'][-1]:.3f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"✅ 모델별 성능(소형 데이터, 참고용)\")\n",
    "m_base   = train(\"Base LSTM\",        build_base())\n",
    "m_drop   = train(\"LSTM + Dropout\",   build_dropout())\n",
    "m_bilstm = train(\"BiLSTM\",           build_bilstm())\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 5) 샘플 예측\n",
    "# ======================\n",
    "samples = [\"정말 감동적이고 훌륭한 영화\", \"지루하고 별로였어 다시 안봐\"]\n",
    "\n",
    "# 문장 → 시퀀스 → 패딩\n",
    "pad_s = pad_sequences(tok.texts_to_sequences(samples), maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "# 모델별 예측 결과 확인\n",
    "for m, tag in [(m_base, \"Base\"), (m_drop, \"Dropout\"), (m_bilstm, \"BiLSTM\")]:\n",
    "    p = (m.predict(pad_s, verbose=0) > 0.5).astype(int).ravel()\n",
    "    print(f\"{tag:>7} 예측:\", list(zip(samples, p)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
