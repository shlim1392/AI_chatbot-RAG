{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96fec59",
   "metadata": {},
   "source": [
    "## 목표: “GPT처럼 다음 단어를 예측하는 Mini Transformer 만들기”\n",
    "\n",
    "아래 코드는 한 파일 안에서 완전히 실행 가능한\n",
    "가장 단순한 “딥러닝 기반 Mini GPT 구조”입니다.\n",
    "(Colab / Jupyter 기준 약 2~3분 실행)\n",
    "\n",
    "### 🧩 1. 데이터: 간단한 문장 학습\n",
    "### 💬 요약 설명\n",
    "\n",
    "| 단계 | 내용 | GPT 학습과의 관계 |\n",
    "| --- | --- | --- |\n",
    "| **1️⃣ 문장 준비** | “I love machine learning” 등 텍스트 데이터 | 문맥(Context) 제공 |\n",
    "| **2️⃣ Tokenizer 학습** | 단어를 정수 ID로 변환 | 단어 단위의 임베딩 입력 |\n",
    "| **3️⃣ 시퀀스 변환** | 문장을 [1,2,3,4] 형태로 | 모델 입력으로 사용 |\n",
    "| **4️⃣ 다음 단계** | 이 시퀀스로부터 “다음 단어” 예측 학습 | GPT의 기본 원리(언어모델링) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cb0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9\n",
      "샘플 시퀀스: [[2, 3, 4, 1], [4, 1, 5, 7], [6, 1, 5, 8], [2, 3, 6, 1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🧩 예시 데이터\n",
    "# GPT(Transformer 기반 언어모델)는 \"이전 단어들을 보고 다음 단어를 예측\"하는 방식으로 학습합니다.\n",
    "# 아래 4개의 짧은 문장을 예시로 사용합니다.\n",
    "# ------------------------------------------------------------\n",
    "texts = [\n",
    "    \"I love machine learning\",\n",
    "    \"machine learning is fun\",\n",
    "    \"deep learning is powerful\",\n",
    "    \"I love deep learning\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🧠 토크나이저 (Tokenizer)\n",
    "# 텍스트 데이터를 숫자(ID)로 바꾸기 위한 도구입니다.\n",
    "# 1) 전체 단어들을 사전(vocabulary)으로 정리\n",
    "# 2) 각 단어를 고유한 정수로 매핑\n",
    "# ------------------------------------------------------------\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# 학습할 단어들을 토크나이저에 등록\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 전체 단어(고유 단어)의 개수 계산\n",
    "# word_index는 {단어: 정수} 딕셔너리 형태\n",
    "vocab_size = len(tokenizer.word_index) + 1   # +1은 padding(0)을 위해 추가\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "# 예시 출력: Vocab size: 9 (단어 8개 + 패딩 1개)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ✨ 문장을 정수 시퀀스로 변환\n",
    "# 'I love machine learning' → [1, 2, 3, 4] 와 같이 숫자열로 바뀝니다.\n",
    "# 각 숫자는 단어의 인덱스를 의미합니다.\n",
    "# ------------------------------------------------------------\n",
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(\"샘플 시퀀스:\", seqs)\n",
    "# 예시 출력:\n",
    "# [[1, 2, 3, 4],\n",
    "#  [3, 4, 5, 6],\n",
    "#  [7, 4, 8, 9],\n",
    "#  [1, 2, 7, 4]]\n",
    "#\n",
    "# 이 상태에서 모델은 예를 들어\n",
    "# [1, 2, 3] → 다음 단어 4(\"learning\")를 맞히는 식으로 학습하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc112d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 시퀀스: (12, 3)\n",
      "타겟 시퀀스: (12, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🧩 입력(input)과 정답(target) 시퀀스를 만들기 위한 빈 리스트\n",
    "# ------------------------------------------------------------\n",
    "input_seqs, target_seqs = [], []\n",
    "\n",
    "# seqs = [[1, 2, 3, 4], [3, 4, 5, 6], ...] 와 같은 숫자 시퀀스\n",
    "for seq in seqs:\n",
    "    # 각 문장을 부분 시퀀스로 나누어 (이전 단어 → 다음 단어) 관계를 학습\n",
    "    # 예: [1, 2, 3, 4]\n",
    "    # → (입력, 타겟)\n",
    "    #   [1] → 2\n",
    "    #   [1, 2] → 3\n",
    "    #   [1, 2, 3] → 4\n",
    "    for i in range(1, len(seq)):\n",
    "        input_seqs.append(seq[:i])  # i번째 이전까지를 입력으로\n",
    "        target_seqs.append(seq[i])  # i번째 단어를 정답(다음 단어)으로\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🧮 가장 긴 시퀀스 길이 계산\n",
    "# 짧은 문장들은 pad_sequences를 이용해 길이를 맞춰줍니다.\n",
    "# ------------------------------------------------------------\n",
    "maxlen = max(len(s) for s in input_seqs)\n",
    "\n",
    "# 입력 데이터 패딩 (짧은 문장은 앞쪽에 0으로 채움)\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, maxlen=maxlen)\n",
    "\n",
    "# 타깃 단어를 원-핫 인코딩(one-hot encoding)\n",
    "# 예: 4 → [0,0,0,1,0,0,0,0,0] (vocab_size 차원)\n",
    "y = tf.keras.utils.to_categorical(target_seqs, num_classes=vocab_size)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🔍 결과 확인\n",
    "# X: (샘플 수, 시퀀스 길이)\n",
    "# y: (샘플 수, 단어 개수) — 다음 단어를 맞히는 분류 문제!\n",
    "# ------------------------------------------------------------\n",
    "print(\"입력 시퀀스:\", X.shape)\n",
    "print(\"타겟 시퀀스:\", y.shape)\n",
    "\n",
    "# 예시 출력\n",
    "# 입력 시퀀스: (12, 3)   → 12개의 훈련 샘플, 길이 3\n",
    "# 타겟 시퀀스: (12, 9)   → 각 샘플의 다음 단어를 9차원 원-핫 벡터로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed982266",
   "metadata": {},
   "source": [
    "### Mini Transformer 블록 정의\n",
    "\n",
    "> GPT는 Decoder 블록을 여러 개 쌓지만, 우리는 1층만 사용합니다.\n",
    "> \n",
    "\n",
    "### 🧠 전체 구조 요약\n",
    "\n",
    "| 블록 | 역할 | GPT 구조에서의 대응 |\n",
    "| --- | --- | --- |\n",
    "| **Embedding** | 단어를 벡터로 표현 | Token Embedding |\n",
    "| **MultiHeadAttention** | 문맥 이해 | Self-Attention Layer |\n",
    "| **LayerNorm + Residual** | 학습 안정화 | Transformer 표준 구성 |\n",
    "| **Feed Forward Network** | 비선형 변환 | FFN 블록 |\n",
    "| **Dropout** | 과적합 방지 | Regularization |\n",
    "| **Dense(vocab_size, softmax)** | 다음 단어 확률 예측 | Output layer |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94a6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 1️⃣ Embedding Layer\n",
    "        # 단어 ID → 임베딩 벡터로 변환 (각 단어를 고정 길이의 벡터로 표현)\n",
    "        # 예: 단어 1 → [0.12, -0.07, 0.55, ...]\n",
    "        # ------------------------------------------------------------\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 2️⃣ Multi-Head Self-Attention\n",
    "        # 입력 문장 내 모든 단어가 서로를 \"참조\"할 수 있도록\n",
    "        # 문맥(Context)을 학습합니다.\n",
    "        # num_heads: 병렬로 주의를 계산할 헤드의 개수\n",
    "        # key_dim: 각 헤드의 차원\n",
    "        # ------------------------------------------------------------\n",
    "        self.attention = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 3️⃣ Feed Forward Network (FFN)\n",
    "        # Attention으로 얻은 정보에 비선형 변환을 추가\n",
    "        # Dense → ReLU → Dense 구조\n",
    "        # ------------------------------------------------------------\n",
    "        self.ffn = Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 4️⃣ Layer Normalization\n",
    "        # 각 단계를 정규화하여 학습을 안정화시킴\n",
    "        # ------------------------------------------------------------\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 5️⃣ Dropout\n",
    "        # 과적합(overfitting) 방지를 위한 정규화 기법\n",
    "        # ------------------------------------------------------------\n",
    "        self.dropout = Dropout(0.2)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 6️⃣ 출력층\n",
    "        # 각 단어가 “다음 단어”일 확률을 예측하는 Softmax 출력\n",
    "        # 출력 차원 = 단어 개수(vocab_size)\n",
    "        # ------------------------------------------------------------\n",
    "        self.out = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # ------------------------------------------------------------\n",
    "        # 순전파(forward pass) 정의\n",
    "        # ------------------------------------------------------------\n",
    "\n",
    "        # 1️⃣ 단어 ID → 임베딩 벡터\n",
    "        x = self.embedding(inputs)\n",
    "        # (배치크기, 문장길이, 임베딩차원)\n",
    "\n",
    "        # 2️⃣ Self-Attention\n",
    "        attn_output = self.attention(x, x)  # (Q, K, V 모두 동일: 자기 자신 참조)\n",
    "\n",
    "        # 3️⃣ 잔차 연결(residual) + 정규화\n",
    "        x = self.layernorm1(x + attn_output)\n",
    "\n",
    "        # 4️⃣ Feed Forward Network\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # 5️⃣ 잔차 연결 + 정규화\n",
    "        x = self.layernorm2(x + ffn_output)\n",
    "\n",
    "        # 6️⃣ Dropout 적용\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 7️⃣ 마지막 단어 위치의 출력만 사용 (GPT는 마지막 토큰 예측)\n",
    "        # x[:, -1, :] → 문장의 마지막 토큰의 벡터만 가져옴\n",
    "        # Dense(vocab_size, softmax)로 “다음 단어” 확률 분포 생성\n",
    "        return self.out(x[:, -1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b294f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.1147\n",
      "Epoch 2/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 2.4457\n",
      "Epoch 3/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5000 - loss: 1.9429\n",
      "Epoch 4/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5000 - loss: 1.5770\n",
      "Epoch 5/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5833 - loss: 1.3123\n",
      "Epoch 6/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5833 - loss: 1.1130\n",
      "Epoch 7/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7500 - loss: 0.9598\n",
      "Epoch 8/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.8387\n",
      "Epoch 9/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7500 - loss: 0.7415\n",
      "Epoch 10/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8333 - loss: 0.6585\n",
      "Epoch 11/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8333 - loss: 0.5869\n",
      "Epoch 12/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8333 - loss: 0.5260\n",
      "Epoch 13/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8333 - loss: 0.4777\n",
      "Epoch 14/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8333 - loss: 0.4402\n",
      "Epoch 15/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.4120\n",
      "Epoch 16/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.3913\n",
      "Epoch 17/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.3749\n",
      "Epoch 18/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.3605\n",
      "Epoch 19/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.3466\n",
      "Epoch 20/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.3318\n",
      "Epoch 21/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.3166\n",
      "Epoch 22/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.3015\n",
      "Epoch 23/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9167 - loss: 0.2875\n",
      "Epoch 24/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9167 - loss: 0.2756\n",
      "Epoch 25/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9167 - loss: 0.2652\n",
      "Epoch 26/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.2567\n",
      "Epoch 27/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9167 - loss: 0.2498\n",
      "Epoch 28/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.2439\n",
      "Epoch 29/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.2384\n",
      "Epoch 30/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.2328\n",
      "Epoch 31/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.2269\n",
      "Epoch 32/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.2206\n",
      "Epoch 33/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.2143\n",
      "Epoch 34/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.2082\n",
      "Epoch 35/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.2024\n",
      "Epoch 36/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9167 - loss: 0.1972\n",
      "Epoch 37/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1923\n",
      "Epoch 38/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1879\n",
      "Epoch 39/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1837\n",
      "Epoch 40/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1797\n",
      "Epoch 41/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1760\n",
      "Epoch 42/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9167 - loss: 0.1726\n",
      "Epoch 43/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1693\n",
      "Epoch 44/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1662\n",
      "Epoch 45/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1633\n",
      "Epoch 46/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9167 - loss: 0.1606\n",
      "Epoch 47/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1582\n",
      "Epoch 48/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1560\n",
      "Epoch 49/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1542\n",
      "Epoch 50/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1526\n",
      "Epoch 51/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9167 - loss: 0.1512\n",
      "Epoch 52/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9167 - loss: 0.1499\n",
      "Epoch 53/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9167 - loss: 0.1487\n",
      "Epoch 54/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1475\n",
      "Epoch 55/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9167 - loss: 0.1463\n",
      "Epoch 56/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1453\n",
      "Epoch 57/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1443\n",
      "Epoch 58/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1435\n",
      "Epoch 59/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1427\n",
      "Epoch 60/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1420\n",
      "Epoch 61/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1414\n",
      "Epoch 62/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1408\n",
      "Epoch 63/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9167 - loss: 0.1402\n",
      "Epoch 64/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1396\n",
      "Epoch 65/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1390\n",
      "Epoch 66/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1384\n",
      "Epoch 67/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1379\n",
      "Epoch 68/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1374\n",
      "Epoch 69/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1370\n",
      "Epoch 70/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1366\n",
      "Epoch 71/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1362\n",
      "Epoch 72/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1358\n",
      "Epoch 73/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1355\n",
      "Epoch 74/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1351\n",
      "Epoch 75/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1347\n",
      "Epoch 76/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1344\n",
      "Epoch 77/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1341\n",
      "Epoch 78/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1338\n",
      "Epoch 79/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1335\n",
      "Epoch 80/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1332\n",
      "Epoch 81/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1330\n",
      "Epoch 82/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1327\n",
      "Epoch 83/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1324\n",
      "Epoch 84/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1322\n",
      "Epoch 85/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1319\n",
      "Epoch 86/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1317\n",
      "Epoch 87/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1315\n",
      "Epoch 88/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9167 - loss: 0.1313\n",
      "Epoch 89/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1311\n",
      "Epoch 90/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1309\n",
      "Epoch 91/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1307\n",
      "Epoch 92/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1305\n",
      "Epoch 93/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1303\n",
      "Epoch 94/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1301\n",
      "Epoch 95/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1299\n",
      "Epoch 96/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1297\n",
      "Epoch 97/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1295\n",
      "Epoch 98/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1294\n",
      "Epoch 99/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1292\n",
      "Epoch 100/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1291\n",
      "Epoch 101/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1289\n",
      "Epoch 102/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1287\n",
      "Epoch 103/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1286\n",
      "Epoch 104/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1284\n",
      "Epoch 105/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1283\n",
      "Epoch 106/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1282\n",
      "Epoch 107/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1280\n",
      "Epoch 108/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1279\n",
      "Epoch 109/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1278\n",
      "Epoch 110/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9167 - loss: 0.1276\n",
      "Epoch 111/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1275\n",
      "Epoch 112/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1274\n",
      "Epoch 113/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1272\n",
      "Epoch 114/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9167 - loss: 0.1271\n",
      "Epoch 115/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1270\n",
      "Epoch 116/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1269\n",
      "Epoch 117/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1268\n",
      "Epoch 118/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9167 - loss: 0.1267\n",
      "Epoch 119/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1266\n",
      "Epoch 120/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1265\n",
      "Epoch 121/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1264\n",
      "Epoch 122/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1263\n",
      "Epoch 123/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1261\n",
      "Epoch 124/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1260\n",
      "Epoch 125/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1260\n",
      "Epoch 126/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1259\n",
      "Epoch 127/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1258\n",
      "Epoch 128/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1257\n",
      "Epoch 129/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1256\n",
      "Epoch 130/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1255\n",
      "Epoch 131/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1254\n",
      "Epoch 132/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9167 - loss: 0.1253\n",
      "Epoch 133/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1252\n",
      "Epoch 134/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1251\n",
      "Epoch 135/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1251\n",
      "Epoch 136/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1250\n",
      "Epoch 137/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1249\n",
      "Epoch 138/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1248\n",
      "Epoch 139/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1247\n",
      "Epoch 140/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1247\n",
      "Epoch 141/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1246\n",
      "Epoch 142/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1245\n",
      "Epoch 143/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1244\n",
      "Epoch 144/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9167 - loss: 0.1244\n",
      "Epoch 145/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1243\n",
      "Epoch 146/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1242\n",
      "Epoch 147/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1241\n",
      "Epoch 148/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1241\n",
      "Epoch 149/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1240\n",
      "Epoch 150/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1239\n",
      "Epoch 151/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1239\n",
      "Epoch 152/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9167 - loss: 0.1238\n",
      "Epoch 153/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9167 - loss: 0.1237\n",
      "Epoch 154/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1237\n",
      "Epoch 155/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1236\n",
      "Epoch 156/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1235\n",
      "Epoch 157/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1235\n",
      "Epoch 158/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1234\n",
      "Epoch 159/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9167 - loss: 0.1234\n",
      "Epoch 160/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9167 - loss: 0.1233\n",
      "Epoch 161/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9167 - loss: 0.1232\n",
      "Epoch 162/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1232\n",
      "Epoch 163/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1231\n",
      "Epoch 164/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1231\n",
      "Epoch 165/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1230\n",
      "Epoch 166/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1230\n",
      "Epoch 167/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1229\n",
      "Epoch 168/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9167 - loss: 0.1229\n",
      "Epoch 169/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9167 - loss: 0.1228\n",
      "Epoch 170/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9167 - loss: 0.1228\n",
      "Epoch 171/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1227\n",
      "Epoch 172/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1227\n",
      "Epoch 173/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9167 - loss: 0.1226\n",
      "Epoch 174/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1226\n",
      "Epoch 175/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9167 - loss: 0.1225\n",
      "Epoch 176/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1225\n",
      "Epoch 177/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1224\n",
      "Epoch 178/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1224\n",
      "Epoch 179/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1223\n",
      "Epoch 180/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1223\n",
      "Epoch 181/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1222\n",
      "Epoch 182/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1222\n",
      "Epoch 183/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1221\n",
      "Epoch 184/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1221\n",
      "Epoch 185/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1220\n",
      "Epoch 186/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1220\n",
      "Epoch 187/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.1220\n",
      "Epoch 188/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9167 - loss: 0.1219\n",
      "Epoch 189/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.1219\n",
      "Epoch 190/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9167 - loss: 0.1218\n",
      "Epoch 191/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1218\n",
      "Epoch 192/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9167 - loss: 0.1217\n",
      "Epoch 193/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9167 - loss: 0.1217\n",
      "Epoch 194/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1217\n",
      "Epoch 195/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9167 - loss: 0.1216\n",
      "Epoch 196/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1216\n",
      "Epoch 197/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1216\n",
      "Epoch 198/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1215\n",
      "Epoch 199/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 0.1215\n",
      "Epoch 200/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.1214\n",
      "✅ 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "model = MiniTransformer(vocab_size, embed_dim=32, num_heads=2, ff_dim=64)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X, y, epochs=200, verbose=1)\n",
    "print(\"✅ 학습 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f3949",
   "metadata": {},
   "source": [
    "### 💬 문장 생성 (GPT처럼 다음 단어 생성)\n",
    "\n",
    "### 작동 과정 정리\n",
    "\n",
    "| 단계 | 설명 | 예시 |\n",
    "| --- | --- | --- |\n",
    "| 1️⃣ 입력 문장 받기 | “I love” | 시드 문장 |\n",
    "| 2️⃣ 숫자로 변환 | [1, 2] | Tokenizer 이용 |\n",
    "| 3️⃣ 모델 예측 | 다음 단어 확률분포 | 예: [0.1, 0.2, 0.6, 0.1] |\n",
    "| 4️⃣ argmax로 단어 선택 | 가장 확률 높은 단어 | “machine” |\n",
    "| 5️⃣ 문장에 추가 | “I love machine” | 반복 수행 |\n",
    "| 6️⃣ 완성 | “I love machine learning fun powerful ...” | 생성 결과 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0471ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 생성 문장 예시:\n",
      "I love machine learning is fun\n",
      "machine learning is fun deep learning\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 🧩 인덱스 → 단어 매핑\n",
    "# Tokenizer는 {단어: 번호} 형태로 저장되어 있기 때문에,\n",
    "# 예측 결과(숫자)를 다시 단어로 바꾸기 위해 역매핑을 만듭니다.\n",
    "# ------------------------------------------------------------\n",
    "index_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🧠 텍스트 생성 함수\n",
    "# seed_text: 시작 문장 (예: \"I love\")\n",
    "# max_words: 생성할 단어 수 (기본값 5)\n",
    "# ------------------------------------------------------------\n",
    "def generate_text(seed_text, max_words=5):\n",
    "    for _ in range(max_words):\n",
    "        # 1️⃣ 입력 문장을 숫자 시퀀스로 변환\n",
    "        seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        # 2️⃣ 학습 시 사용한 maxlen(문장 최대 길이)에 맞게 패딩\n",
    "        seq = tf.keras.preprocessing.sequence.pad_sequences([seq], maxlen=maxlen)\n",
    "\n",
    "        # 3️⃣ 현재 문맥(seq)을 모델에 입력 → 다음 단어의 확률 예측\n",
    "        pred = np.argmax(model.predict(seq, verbose=0))\n",
    "        # np.argmax: 확률이 가장 높은 단어의 인덱스를 선택\n",
    "\n",
    "        # 4️⃣ 인덱스를 실제 단어로 변환\n",
    "        next_word = index_to_word.get(pred, '')\n",
    "\n",
    "        # 5️⃣ 예측된 단어를 문장에 이어붙임\n",
    "        seed_text += ' ' + next_word\n",
    "\n",
    "    # 6️⃣ 모든 단어를 생성한 후 완성된 문장 반환\n",
    "    return seed_text\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🔍 예시: 두 개의 시작 문장으로 새 문장 생성\n",
    "# ------------------------------------------------------------\n",
    "print(\"🧩 생성 문장 예시:\")\n",
    "print(generate_text(\"I love\", max_words=4))\n",
    "print(generate_text(\"machine learning\", max_words=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad3f73",
   "metadata": {},
   "source": [
    "> 모든 단어가 나오는것은 확률싸움인것  \n",
    "> 할루시네이션 발생가능성  \n",
    "> 올바른 체계를 구성해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280dcde",
   "metadata": {},
   "source": [
    "## 📘 실습 정리표\n",
    "\n",
    "| 단계 | 개념 | 실습 코드 |\n",
    "| --- | --- | --- |\n",
    "| 1 | 단어를 숫자로 변환 (Tokenization) | `Tokenizer()` |\n",
    "| 2 | 다음 단어 예측을 위한 데이터셋 구성 | Sliding window |\n",
    "| 3 | Self-Attention 계산 | `MultiHeadAttention()` |\n",
    "| 4 | 문장 생성 | 반복적으로 next word 예측 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f4e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72de8f6c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
